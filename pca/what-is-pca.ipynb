{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf7f3ab-3bbc-418c-bda9-1a2534721e41",
   "metadata": {},
   "source": [
    "# PCA for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda7ea4-d10a-4ec2-8a7a-0a0294400fb0",
   "metadata": {},
   "source": [
    "In this notebook we explore Principal Component Analysis (PCA) and how it can be used to interpret high dimensional datasets in lower dimensions. \n",
    "To do so, we review concepts from linear algebra to get a fuller understanding of what's going on and why PCA works.\n",
    "We then walk through running PCA on a small dataset. \n",
    "\n",
    "We first review how PCA is defined, what exactly the principal components are, and how PCA allows us to reduce the dimensionality of our dataset in a *good* way. \n",
    "We then see how we can use PCA in Python by analyzing a [diabetes dataset](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html) provided by NCSU\n",
    "and what the principal components are. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efaa61d-d326-46b1-9a22-6b49659a4a69",
   "metadata": {},
   "source": [
    "Suppose we have a dataset of $n$ samples where each sample is described by $m$ real valued attributes. \n",
    "For the purpose of dimensionality reduction, our goal will be to find a matrix $Y$ of size $n \\times d$ where $d << m $ and $Y$ approximates $X$ \n",
    "(that is, $Y$ still captures the underlying properties of the dataset.)\n",
    "In this way, the $d$ remaining attributes would be the componenets of $X$ that capture the most relevant information of our dataset, or are its **principal components**.\n",
    "\n",
    "Before we see how we can compute these componenets, let's consider some scenarios where such a reduction can occur. \n",
    "One possibility is a set of attributes within the dataset are the same for each sample - there is no information that these attributes have that allow us to distinguish one sample from another. \n",
    "Therefore we could easily remove this set of attributes shrinking the size of $X$.\n",
    "Typically this is not the case however and there is some variability between sample values for a particular attribute. \n",
    "An attribute that has less variability amongst its values may contain less information than an attribute with high variability and therefore could be removed from $X$ to generate $Y$.\n",
    "\n",
    "Another possiblity is there are some dependency between attributes within the data. \n",
    "For example, if we find $x_i = x_j + x_k$ for all samples and attributes $i,j,k$ then attribute $i$ is dependent on $j$ and $k$ and could be removed.\n",
    "In this way, there is some **correlation** between our attributes, and while a perfect correlation may not always exist due to noisy measurements, we may be able to drop many attributes without losing too much information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c0d34d-c217-448a-92e8-adea6f35abcc",
   "metadata": {},
   "source": [
    "## Principal Components of a Dataset \n",
    "\n",
    "Suppose we have a dataset of $n$ samples. Each sample is measured by one of $m$ attributes.\n",
    "We can interpret this as a matrix $X$ of size $n \\times m$  where each row vector corresponds to a sample. \n",
    "For the purpose of dimensionality reduction, we would like to find a matrix $Y$ of size $n \\times d$ where $d << m $ and $Y$ approximates $X$ \n",
    "(that is, $Y$ still captures the underlying properties of the dataset.)\n",
    "In this way, the $d$ remaining attributes would be the componenets of $X$ that capture the most relevant information, or are the **principal components** of $X$.\n",
    "\n",
    "Before we see how we can compute these componenets, let's consider some scenarios where such a reduction can occur. \n",
    "One possibility is a set of attributes within the dataset are the same for each sample - there is no information that these attributes have that allow us to distinguish one sample from another. \n",
    "Therefore we could easily remove this set of attributes shrinking the size of $X$.\n",
    "Typically this is not the case however and there is some variability between sample values for a particular attribute. \n",
    "An attribute that has less variability amongst its values may contain less information than an attribute with high variability and therefore could be removed from $X$ to generate $Y$.\n",
    "\n",
    "Another possiblity is there are some dependency between attributes within the data. \n",
    "For example, if we find $x_i = x_j + x_k$ for all samples and attributes $i,j,k$ then attribute $i$ is dependent on $j$ and $k$ and could be removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea63b96-155b-48b9-b63d-62a85e83d4f0",
   "metadata": {},
   "source": [
    "## Linear Algebra Recap: SVD, Eigenpairs, etc.\n",
    "\n",
    "To make the steps to PCA clearer, we first review some concepts from linear algebra. \n",
    "We do not aim to prove any statements here but encourage the reader to dive deeper into them using resources like\n",
    "Gilber String's introduction to linear algebra books or Brunton and Kutz's [Data Drive Science & Engineering](https://databookuw.com/databook.pdf) book.\n",
    "\n",
    "### Means and Variance\n",
    "\n",
    "Given $n$ samples and $m$ attribute vectors, we can compute what the **mean value** is for each attribute is, and compare each value to how it differs from the mean. \n",
    "We can then define what the variance for each attribute is. \n",
    "However we can further extend this by considering what the variance is between any two attributes. \n",
    "These are combined into a single $m \\times m$ matrix called the **covariance** matrix. \n",
    "\n",
    "If our dataset is mean centered (the mean rows of $X$ are 0) it follows that the covariance matrix of $X$ is\n",
    "$$\n",
    "C = \\frac{1}{n-1}X^TX\n",
    "$$\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "The SVD of a matrix $X$ is a decomposition of $X$ into 3 special matrices:\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "where \n",
    "- $U$ is the left singular value vectors, a $n \\times n$ matrix,\n",
    "- $\\Sigma$ is a real, non-negative valued diagonal matrix (zeros off the main diagonal - contains 0s below if $n \\geq m$), a $m \\times n$ matrix, and\n",
    "- $V$ is the right singular value vectors, a $m \\times m$ matrix.\n",
    "\n",
    "There are other notable properties are:\n",
    "1. The order of $u_1, u_2, ..., u_n$ is such that $u_1$ describes the variance in the cols of $X$ more than $u_2, ..., u_n$, $u_2$ describes the variance in the cols of $X$ more than $u_3, ...$, and so on.\n",
    "2. $U$ and $V$ are unitary matrices such that $U^TU = V^TV = I$ (the identity matrix.)\n",
    "3. The entries of $\\Sigma$ are $\\sigma_1, \\sigma_2, ...$ and are ordered such that $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq 0$.\n",
    "4. Columns within $U$ and $V$ are orthonormal.\n",
    "\n",
    "We do not concern ourselves with how the decomposition can be calculated (Python's `numpy` package provides our implementation) only that such a decomposition exists. \n",
    "\n",
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "For a matrix $A$ of size $n \\times n$, the vector $v$ and value $\\lambda$ are called eigenvector and eigenvalue if\n",
    "$$\n",
    "Av = \\lambda v\n",
    "$$\n",
    "A key property of eigenvectors are vectors which are only changes by a scalar value whenever a linear transformation is applied to them.\n",
    "\n",
    "If we stack all the eigenvectors together into a $n \\times n$ matrix $V$ we have\n",
    "$$\n",
    "AV = V \\Lambda\n",
    "$$\n",
    "where $\\Lambda$ is a diagonal matrix with matrix filled with eigenvalues.\n",
    "\n",
    "## SVD and PCA\n",
    "\n",
    "We now see how the SVD of our dataset relates to its principal components. \n",
    "First we mean center our dataset. We then compute $X^TX$ using the SVD of $X$\n",
    "$$\\begin{aligned}\n",
    "X^TX &= (U \\Sigma V^T)^T (U \\Sigma V^T) \\\\\n",
    "&= V \\Sigma^T U^T U \\Sigma V^T \\\\\n",
    "&= V \\Sigma^T \\Sigma V^T \\\\\n",
    "&= V D V^T\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $D = \\Sigma^T \\Sigma$ is a diagonal with squares of singular values and making note of $U$ being a unitary matrix. \n",
    "\n",
    "Now we compute $(X^TX)V$ using the above\n",
    "\n",
    "$$\\begin{aligned}\n",
    "(X^TX)V &= (V D V^T) V \\\\\n",
    "(X^TX)V &= V D\n",
    "\\end{aligned}$$\n",
    "as $V$ is a unitary matrix. \n",
    "\n",
    "We now observe that $V$ is the eigenvectors of, not our dataset $X$, but $X^TX$ and that $D$ contains our eigenvalues (which are $\\sigma_1^2, \\sigma_2^2, ...$).\n",
    "But since our dataset is mean centered, we know that $X^TX = (n-1)C$ our dataset's covariance matrix scaled by $n-1$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0726a7e5-b476-42fe-8d00-c4c4477fc5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "dia = load_diabetes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b565e807-bb25-4642-904a-77bf3ffa431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dia.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5d02f16-ba91-4c3c-8fb2-3834f3654db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dia.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "003f9307-14d8-4ae7-95c0-c5ad58bd7497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1072256316073538"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dia.data[:, 0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caebe557-0fac-476d-a130-a28be69cfc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621,  0.02187239, -0.0442235 ,\n",
       "        -0.03482076, -0.04340085, -0.00259226,  0.01990749, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, -0.02632753, -0.00844872,\n",
       "        -0.01916334,  0.07441156, -0.03949338, -0.06833155, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, -0.00567042, -0.04559945,\n",
       "        -0.03419447, -0.03235593, -0.00259226,  0.00286131, -0.02593034],\n",
       "       [-0.08906294, -0.04464164, -0.01159501, -0.03665608,  0.01219057,\n",
       "         0.02499059, -0.03603757,  0.03430886,  0.02268774, -0.00936191],\n",
       "       [ 0.00538306, -0.04464164, -0.03638469,  0.02187239,  0.00393485,\n",
       "         0.01559614,  0.00814208, -0.00259226, -0.03198764, -0.04664087]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dia.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305efee4-0187-448a-8d93-8c9745c446f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
